---
blog: true
---

## 摘要

CubeFS Release 3.2.1 版本发布于2023年3月16日，本版更新内容较多，涉及几个大 feature 的新增，部分稳定性优化以及 bugfix。

**主要Feature：**
- 支持目录文件数目配额，防止超大目录导致 MetaNode 中 MP 内存消耗过大。
- 数据分区 (data partition) 下线迁移并发速度控制。
- master 接口限流，防止瞬间高并发导致过载。
- 操作审计日志
- S3 对接纠删码子系统 (Blobstore)

**主要Bugfix：**

- master snapshot 同步时，只从 leader 添加了元数据信息，导致已经在 leader 上删除了的元数据没有清理。
- data partition 数过多时，master 重启可能消耗大量内存，存在 OOM 风险。
- 大量 extent 清理请求可能会导致 datanode tcp 连接过载。

版本地址：https://github.com/cubefs/cubefs/releases/tag/v3.2.1

## 特性介绍

### 支持目录文件数目配额

限制单个目录下子文件数或者目录数，避免文件数过多导致父目录所在的 meta partition 内存占用过大，引发 meta 节点内存使用不均衡，易导致 MetaNode 出现 OOM 。

- 每个目录的默认孩子数quota为2千万，可以配置，最小值为1百万，无上限。当单个目录下创建的孩子超过quota，则创建失败。
- 配置的quota值对整个集群生效，持久化在master。
- 完整支持需要Client，MetaNode，Master均升级到3.2.1版本。

### 数据分区下线限速

由于数据分区下线过程中会优先从数据分区所在源节点/磁盘的 nodeset 中挑选一个目标节点来作为目标节点，来迁入数据分区。因此每个 nodeset 会包含一个下线列表 decommissionList ，定期(5S)依次下线其中的数据分区以实现限速的目的。

以nodeset为单位，限制数据分区data partition并行下线的个数

![ar](/images/blog/v32101.png)

`DecommissionDataPartitionList` 是一个 FIFO 的队列。当数据分区完成下线（成功/失败）或者停止时，会从队列中删除。

`DecommissionDataPartitionList`支持数据分区并行下线的最大数目是可以配置的。

配置接口: `/admin/updateDecommissionLimit`

### Master接口限流

为了防止瞬间高并发的请求到达master导致不可用的情况，现在所有接口都支持配置限流控制。

- 接口级别限流，可以控制指定接口限流值，不配置则无限流。
- 可以配置限流后接口的等待时间，超过等待时间则接口返回429响应码，防止雪崩效应。

### 操作审计日志

挂载点的操作审计流水，审计日志存放在客户端本地指定目录，方便接入第三方的日志采集平台。

- 客户端可以自主选择开始或者关闭审计功能，无需重新挂载。
- 审计格式：
```shell
[集群名（master域名或者ip），卷名，subdir，mountpoint，时间戳，clientip，client hostname，操作类型op (创建、删除、Rename)，源路径，目标路径，错误信息，操作耗时，源文件inode, 目的文件inode]
```

### S3对接纠删码子系统 (blobstore)

- 支持通过S3协议访问纠删码卷

## 后续版本计划

3.2.2 版本更新预告：
- Posix 接口原子性
- 目录空间配额
- 用户 uid 配额
- 运维和稳定性提升
- Blobstore 在 Proxy 节点缓存卷和磁盘信息，优化后台任务并发策略。

来自BIGO团队的：
- M1eyu, @M1eyu2018
- litao, @tomscut
- qinyuren, @liubingxing

来自OPPO团队:
- Patrick Wu，@wuchunhuan
- Whale Tang，@true1064
- baijiaruo, @baijiaruo
- Victor1319, @Victor1319
- leonchang，@leonrayang
- zhangtianjiong,@zhangtianjiong
- AmazingChi, @bboyCH4